#set page(numbering: "1")

#set text(
  font: "New Computer Modern",
  size: 12pt,
  lang: "de",
)

#set par(justify: true)

#set heading(numbering: "1.")

#let colorred(x) = text(fill: red, $#x$)
#let colorblue(x) = text(fill: blue, $#x$)

#align(center, [
  #text(size: 28pt)[
    Eigenwerte und Eigenvektoren \ Basics und Anwendungen
  ]
  \ \
  Michael Winsauer - #link("mailto:michael.winsauer@study.thws.de", [michael.winsauer\@study.thws.de])  \
  GÃ¼lsÃ¼m ? - #link("mailto:?.?@study.thws.de", [?.?\@study.thws.de]) \
  Luis Weich - #link("mailto:luis.weich@study.thws.de", [luis.weich\@study.thws.de])
])

\ \

#outline(indent: auto)

#pagebreak()

#outline(
  title: [Abbildungen],
  target: figure.where(kind: image),
)

#pagebreak()

= *Eigenwerte und Eigenvektoren*

== EinfÃ¼hrung (Luis)

Eigenwerte und Eigenvektoren spielen nicht nur in der linearen Algebra,
sondern auch in zahlreichen Bereichen unseres Lebens eine zentrale Rolle.
Ob in der Mechanik oder in der Physik oder â€“ wie wir spÃ¤ter sehen werden â€“ sogar
in unserem greifbaren Alltag:
Die Reihenfolge unserer Google-Suchergebnisse lÃ¤sst sich als ein sortierter
Eigenvektor darstellen.
#footnote[Heutzutage verwendet Google vielfach erweiterte und komplexere Algorithmen um
unsere Suchergebnisse zu berechnen. Viele weitere Faktoren spielen eine zentrale Rolle:
z.B. Inhalt, Ladezeit, Werbung, KI und weiteres.]

Diese scheinbar abstrakten mathematischen Konzepte helfen uns dabei,
komplexe Systeme zu verstehen, Muster zu erkennen und Prozesse effizient zu gestalten.

Wenn wir eine quadratische Matrix $M in RR^(n crossmark n)$ und
einen Spaltenvektor $arrow(v) in RR^n$ multiplizieren bekommen wir als Ergebnis wieder einen
Spaltenvektor. Dieser Spaltenvektor zeigt jedoch meistens in ganz verschiedene Richtungen
und hat mit dem ursprÃ¼nglichen Vektor $arrow(v)$ nichts mehr gemeinsam.
Manchmal jedoch bekommen wir einen Vektor als Ergebnis, der sich als vielfaches
von dem ursprÃ¼nglichen Vektor $arrow(v)$ darstellen lÃ¤sst.

Allgemein lÃ¤sst sich sagen, dass jeder Vektor $arrow(v)$, den man mit einer
Matrix $M$ multipliziert und dessen
Ergebnis ein vielfaches von dem ursprÃ¼nglichen Vektor $arrow(v)$ ist,
nennt man *Eigenvektor*. Die dazugehÃ¶rige Zahl $lambda$ mit der der Vektor $arrow(v)$ im 
Ergebnis multipliziert bzw. skaliert wird nennt man *Eigenwert*.

Wenn wir eine Gleichung in der folgenden Form vor uns haben, kÃ¶nnen wir Ã¼ber
das _"Eigenwertproblem"_ sprechen:

#text(size: 20pt)[
  $ M arrow(v) = lambda arrow(v) $
]

Hierbei gilt
$M in RR^(n crossmark n)$,
$arrow(v) in RR^n$ mit $arrow(v) eq.not 0$ und
$lambda in RR$.

Im Folgenden werden wir das Eigenwertproblem erklÃ¤ren, Beispiele zeigen und
anschaulich machen.
AuÃŸerdem werden wir Anwendungen aus dem echten Leben zeigen und erklÃ¤ren.
Begleitend dazu zeigen wir Python Code Beispiele.

#pagebreak()

== Geometrische Anschauung (Luis)

Geometrisch betrachtet werden Eigenvektoren durch die Transformation lediglich in ihrer LÃ¤nge
verÃ¤ndert.
Ihre Richtung bleibt bei positivem Eigenwert erhalten, kehrt sich bei negativem Eigenwert jedoch um. Man kann sagen der

$
"Vektor wird" cases(
  "gestreckt " & "fÃ¼r " lambda > 1 \
  "gestaucht " & "fÃ¼r " 0 < lambda < 1 \
  "nicht verÃ¤ndert " & "fÃ¼r " lambda = 1 \ 
  "zu" 0 & "fÃ¼r " lambda = 0
)
$

Ist $lambda < 0$ wird dazu noch die Richtung umgekehrt.

Diese LÃ¤ngenunterschiede ergeben sich durch eine Skalierung, die sich durch die
Gleichung $arrow(v)_text("skaliert") = lambda arrow(v)$ beschreiben lÃ¤sst.

In @plot_eigen1 wird beispielsweise die Gleichung
$mat(2, 1; 1, 2) dot vec(1, 1) = 3 dot vec(1, 1)$
dargestellt.

#figure(
  image("plot_eigen1.png", width: 80%),
  caption: [Vektor und skalierter Vektor in $RR^2$],
) <plot_eigen1> \

Zur Veranschaulichung wurden die Vektoren in diesem Beispiel
im $RR^2$ dargestellt, grundsÃ¤tzlich ist das Konzept jedoch auch auf $RR^n$ Ã¼bertragbar.

#pagebreak()

== Formel (GÃ¼lsÃ¼m)

Gegeben sei eine reelle quadratische nÃ—n-Matrix $ AâˆˆR^(nÃ—n)$.

Gesucht sind

- ein vom Nullvektor verschiedener Vektor $xâˆˆR^n$ $ğ‘¥â‰ 0$, und

- ein Skalar Î»âˆˆR (wobei auch Î»=0 erlaubt ist),
welche die Gleichung

*$ A x = lambda x $*
erfÃ¼hlen.

Ein solches Î» heiÃŸt *Eigenwert*  von A, ein passendes x heiÃŸt *Eigenvektor* von
A zum Eigenwert Î».

//#highlight(extent: 4pt )

 

*Bemerkung*

*1.* Der Nullvektor (0) ist kein zulÃ¤ssiger Eigenvektor, da die Gleichung

 $ A 0 = lambda 0 $  
                                                
fÃ¼r jede Matrix A und jede Zahl Î» erfÃ¼llt ist und somit keinerlei Erkenntnisgewinn bringt   

*2.* Ein Eigenvektor ist nicht eindeutig bestimmt. Wenn v ein Eigenvektor zu einem Eigenwert Î» von A ist, so ist auch jeder skalare Vielfache $Î± v$ mit $ğ›¼â‰ 0$ ein Eigenvektor, da
$ A(Î± v)=Î±(A v)=Î±(Î» v)=Î»(Î± v) $  

== LÃ¶sungsverfahren (GÃ¼lsÃ¼m)
=== Berechnung der Eigenwerte

Um die Eigenwerte einer Matrix $AâˆˆR^(nÃ—n)$ zu bestimmen, verwenden wir die Gleichung

$ A v = Î» v $(1.1)

mit einem Eigenvektor $ğ‘£â‰ 0$ und dem Eigenwert Î»âˆˆR.

 formen wir
(1.1) um, indem wir die n Ã— n - Einheitsmatrix $E_n$ verwenden:

$ A v = Î» v <==> A v = Î» E_n v $  
$ <==> A v âˆ’ Î» E_n v = 0 $
$ <==> (A âˆ’ Î» E_n)v = 0. $ 

Mit bekannten Resultaten uber die LÃ¶sbarkeit von quadratischen
linearen Gleichungssysteme erhalten wir:

$ Î» "ist Eigenwert von" A <==> "Ker"(A âˆ’ Î» E n) "ist nicht-trivial" $  

Das bedeutet: Ein Skalar Î» ist Eigenwert von A, wenn der Kern (Nullraum) der Matrix $Aâˆ’Î» E_n$ nicht-trivial ist, also mehr als nur den Nullvektor enthÃ¤lt.

$ <==> A âˆ’ Î» E n "ist nicht invertierbar" $ 

Eine Matrix ist invertierbar genau dann, wenn ihr Kern nur den Nullvektor enthÃ¤lt. Da hier der Kern nicht-trivial ist, ist $Aâˆ’Î» E_n$

$ <==> det(A âˆ’ Î» E n) = 0 $  
Eine Matrix ist genau dann invertierbar, wenn ihre Determinante ungleich Null ist. Ist die Determinante Null, so ist die Matrix singulÃ¤r und damit nicht invertierbar.

\
Daher gilt fÃ¼r die Eigenwerte Î» von A die sogenannte charakteristische Gleichung:
$ det(A âˆ’ Î» E n) = 0 $ 

Diese Gleichung stellt ein Polynom n-ten Grades in Î» dar, das *charakteristische Polynom* von A genannt wird.
$ #table(  [$P_A$(Î») = $det(A âˆ’ Î» E_n)$ ] )  $ 

Um die Eigenwerte zu bestimmen, lÃ¶st man dieses Polynom nach 
Î» auf.

Da das charakteristische Polynom vom Grad n ist, hat es hÃ¶chstens 
n Nullstellen. Das bedeutet, dass die Matrix A hÃ¶chstens n Eigenwerte besitzt.

*Beispiel 1*

Ermitteln Sie die Eigenwerte fÃ¼r die Matrix
$ A = mat(1,2;
3,4;) $

 Berechnen wir nun Aâˆ’Î»Â·I:
 
  A âˆ’ Î» Â· I = $ mat(1-Î», 2;
              3, 4âˆ’Î»;)$ 
              
Die Determinante ist:

det(A âˆ’Î»Â·I) = (1âˆ’Î»)(4âˆ’Î»)âˆ’2Â·3 

Nun haben wir die charakteristische Gleichung:

$ Î»^2 âˆ’ 5 Î» âˆ’ 2 = 0 $  


Wir lÃ¶sen dieses Polynom, um die Eigenwerte zu finden DafÃ¼r wenden wir die Mitternachtsformel an:

$ lambda = (âˆ’b Â± sqrt(b^2 - 4 dot a dot c) ) /
( 2 dot a) $

$ lambda = (5 Â± sqrt((-5)^2 - 4 dot 1 dot -2) ) /
( 2 dot 1) $

$ lambda = (5 Â± sqrt(25+8) ) /
( 2 ) $
$ lambda = (5 Â± sqrt(33) ) /
( 2 ) $
$ lambda = (5 Â± 5.744) /
( 2 ) $


 $ Î» 1 = 5.561, Î» 2 = âˆ’0.561 $
 
 Die Eigenwerte der Matrix A sind:
 $ Î» 1 = 5.561, Î» 2 = âˆ’0.561 $

=== Bestimmung der Eigenvektoren
Um einen Eigenvektor zu berechnen, braucht man zuerst den zugehÃ¶rigen Eigenwert.


- Fur jeden Eigenwert  Î» mussen nun alle LÃ¶sungen $vâ‰ 0$ des homogenen LGS
$ #table([(A âˆ’ Î»$E_n$)v = 0]) $
gefunden werden. Diese sind die Eigenvektoren von A zum Eigenwert Î»

- Wir lÃ¶sen das System mit einem Verfahren wie dem GauÃŸ-Algorithmus.

- Wenn die Matrix Aâˆ’Î»I nicht invertierbar (singulÃ¤r) ist, hat das System keine eindeutige LÃ¶sung.

  In diesem Fall wÃ¤hlen wir eine der Unbekannten beliebig und berechnen die anderen daraus
\
=== Einfaches Beispiel
   Ermitteln Sie die Eigenwerte fÃ¼r die Matrix $ mat(0, 2; 2, 3;)$  
   Die charakteristische Gleichung gibt uns
   $ mat(0-Î», 2;
        2, 3-Î») =0 $

Deshalb haben wir
$ âˆ’Î»(3âˆ’Î»)âˆ’4=0â‡’Î» 2âˆ’3Î»âˆ’4=0  $

Wir erhalten zwei Eigenwerte:
$ Î»_1=4, Î»_2=âˆ’1$

*VERSUCHEN SIE ES!* Ermitteln Sie die Eigenvektoren fÃ¼r die beiden obigen Eigenwerte.

*a)*  $lambda_1$  = 4:

Um den ersten Eigenvektor zu bestimmen, setzen wir $lambda_1$  = 4 in die Gleichung (Aâˆ’Î»I)v=0 ein:

$ mat(0-4, 2;
        2, 3-4) dot vec(x_1,x_2) = vec(0,0)  $

$ mat(-4,2;
      2, -1;) dot vec(x_1,x_2) = vec(0,0) $

Dies ergibt zwei Gleichungen:
$ -4 x_1 + 2 x_2 = 0 $
$ 2 x_1 -x_2 = 0 $
Beide Gleichungen zeigen, dass $x_2$ = $2x_1$ ist. Daher kann der erste Eigenvektor wie folgt dargestellt werden:

$ v_1 = k_1 vec(1,2) $

Hierbei ist $k_1$ ein Skalar (mit $k_1 eq.not 0$), solange das VerhÃ¤ltnis zwischen $x_2$ und $x_1$ gleich 2 ist, handelt es sich um einen Eigenvektor.

â€‹Wir kÃ¶nnen Ã¼berprÃ¼fen, ob der Vektor $vec(1,2)$ ein Eigenvektor ist, indem wir ihn in die Matrix einsetzen:
$ mat(0, 2; 2, 3;) dot vec(1,2) = vec(4,8) = 4 vec(1,2) $ 

*b)*  $lambda_2$ = -1
Auf Ã¤hnliche Weise setzen wir $lambda_2 = -1 $ ein und erhalten den zweiten Eigenvektor: 

$(A - Î» I) dot v = 0$ 
$ mat(0, 2; 2, 3;) dot vec(x_1, x_2) = vec(0,0)$  

Dies ergibt das Gleichungssystem: 
1. $x_1 + 2x_2 = 0$  
2. $2x_1 + 4x_2 = 0$  

Da die zweite Gleichung ein Vielfaches der ersten ist, betrachten wir nur die erste:  
$ x_1 = -2x_2 $
$ v_2 = k_2 vec(-2,1) $ wobei $k_2 eq.not 0 $

Was ist der Ansatz um ein Eigenwertproblem zu berechen?

=== Komplexeres Beispiel

Gegeben sei die Matrix
$ A = mat(1, 2, -1;
      0,3,0;
      -1,2,1;) $
Berechnen Sie die Eigenvektoren.

*LÃ¶zung:*

_Schritt 1: Eigenwerte berechnen:_

1.1) Bilde A - Î».I

1.2) Berechne det(A- Î».I)

1.3) Nulstellen von det(A-Î».I)


$  A - Î».I = mat(1âˆ’Î»,2,-1;
                    0,3âˆ’Î» ,0;
                    -1,2,1-Î»;) $
                    
 Determinante berechnen:
 
$   det(A- Î».I) = (1âˆ’Î»).  det mat(3âˆ’Î» ,0;
2,1âˆ’Î»;) - 0 dot det mat(2,-1;
2, 1âˆ’Î»;) +(-1) dot mat(2,-1;
3-Î»,0;) $ 

$ = (1âˆ’Î») dot [(3-Î») dot (1-Î»)- 2 dot 0]  + (-1) dot [2 dot 0 - (-1) (3-Î»)] $

$ = (1-Î»)dot(3-Î»)dot(1-Î») - 0 +(-1)dot(3-Î») $  

$ = (1-Î»)dot(3-Î»)dot(1-Î») -(3-Î») $

$ = (3-Î»)(1-Î»)^2-1 $

 Nulstellen von det(A-Î».I)

 $ (3-Î»)(1-Î»)^2-1 = 0 $
 $ (3-Î»)(1-2Î» +Î»^2 -1)=0 $
 $ (3-Î»)Î»(-2+Î»)=0 $

 Die Eigenwerte sind:

 $  lambda_1=0, lambda_2=3, lambda_3=2 $  

_Schritt 2: Eigenvektoren berechnen_

FÃ¼r jeden Eigenwert Î» lÃ¶sen wir das Gleichungssystem:

$ (A - Î» I) dot v = 0 $

*FÃ¼r Î» = 0*

$ (A-lambda_1I)x = mat((1-0),2,-1;
                         0,(3-0),0;
                         -1,2,1-0;) vec(x_1,x_2,x_3) = vec(0,0,0) $
 

LÃ¶sung linearer Gleichungssysteme mit dem GauÃŸ-Algorithmus

$ cases(x_1+2_x_2-x_3=0,
3x_2=0,
-x_2+2x_2+x_3=0) $  


Erweiterte Matrix:
$  mat(1, 2, -1,|0;
      0,3,0,|0;
      -1,2,1,|0;) $
      

GauÃŸsche Eliminierung:
      $ R 3= R 1 + R 3 ==> mat(1,2,-1,| 0;
                              0,3,0,| 0;
                            0,4,0,| 0;) $

    $ R 3 \u{0027} = 4 dot R 2 -3  dot R 3 ==>mat(1,2,-1,| 0;
            0,3,0,| 0;
            0,0,0,| 0;) $
            
LÃ¶sung des Gleichungssystems:

 $x_1 +2 dot x_2 -1dot x_3=0 ==> x_1 +2x_2 -x_3=0 $
 
 $3x_2=0 ==> x_2=0$
 
 Einsetzen in die erste Zeile:
 
 $x_1+2 dot 0-x_3=0 ==> x_1-x_3=0 ==> x_1=x_3 $

 Da $x_3$ eine freie Variable ist, setzen wir $x_3$= t (mit t âˆˆ R).
 
Also:

$x_1=t, x_2=0, x_3=t $ fÃ¼r eine beliebiges t âˆˆ R \u{005C}{0}

    $ v_1 = t vec(1,0,1) $

    
*FÃ¼r $ Î»_2 = 3$*
$ (A-lambda_1I)x = mat((1-3),2,-1;
                         0,(3-3),0;
                         -1,2,(1-3);) vec(x_1,x_2,x_3) = vec(0,0,0) $

Dies ergibt die reduzierte Matrix:
 $                   mat(-2,2,-1;
                         0,0,0;
                         -1,2,-2;) vec(x_1,x_2,x_3) = vec(0,0,0) $
Lineares Gleichungssystem: 
$ cases(-2x_1+2x_2-x_3 = 0,
         0=0,
        -x_1+2x_2-2x_3 = 0,
         ) $
Erweiterte Matrix:
$ mat(-2,2,-1,|0;
     0,0,0,|0;
     -1,2,-2,|0;)  $     

GauÃŸsche Eliminierung:



$ R 3 = R 3- 1/2R 1$

$(-1) -1/2  (-2) = -1 +1=0$

$ 2- 1/2 dot 2 = 2-1 =1$    
$-2-1/2(-1)=-2+1/2 = -3/2$


$ ==> mat(-2,2,-1,|0;
     0,0,0,|0;
     0,1,-3/2,|0;)  $  



R1 \u{0027} = -1/2.R1
$ ==> mat(1,-1,1/2,|0;
     0,0,0,|0;
     0,1,-3/2,|0;)  $ 

LÃ¶sung des Gleichungssystems:

1.Gleichung:

$x_1-x_2+1/2x_3=0 ==> x_1=x_2-1/2x_3$

2.Gleichung:

$x_2-3/2x_3=0 ==> x_2=3/2x_3$

Da $x_3$ eine freie Variable ist, setzen wir $x_3$= z (mit z âˆˆ R).

$x_2=3/2z$

$x_1=3/2z -1/2z=z$

Der Eigenvektor ist somit:

$ v_2=z vec(1,3/2,1) $


*FÃ¼r $lambda_3 = 2 $*

$ (A-lambda_1I)x = mat((1-2),2,-1;
                         0,(3-2),0;
                         -1,2,(1-2);) vec(x_1,x_2,x_3) = vec(0,0,0) $

Dies ergibt die reduzierte Matrix:
 $                   mat(-1,2,-1;
                         0,1,0;
                         -1,2,-1;) vec(x_1,x_2,x_3) = vec(0,0,0) $
Lineares Gleichungssystem: 
$ cases(-1x_1+2x_2-x_3 = 0,
         1x_2=0,
        -x_1+2x_2-1x_3 = 0,
         ) $

LÃ¶sung des Gleichungssystems:

Zweite Gleichung:

$x_2=0$

Einsetzen von $x_2=0$ in die beiden anderen Gleichungen:

Erste Gleichung:

$-1x_1+2.0-1x_3=0 ==> -x_2-x_3=0$

$x_1=-x_3$

Dritte Gleichung:

$-1x_1+2 dot 0 -1x_3=0 ==> -1x_1-x_3=0 $  

Dies ist die gleiche Gleichung wie die erste, also gibt es keine zusÃ¤tzlichen Informationen.
Da $x_3$ eine freie Variable ist, setzen wir 
   $x_1$= -k, $x_2=0$, $x_3=k$ 

Der Eigenvektor ist somit:

$ v_3=k vec(-1,0,1) $

Eigenvektoren:

$ v_1 = t vec(1,0,1) , v_2=z vec(1,3/2,1) ,v_3=k vec(-1,0,1) $

Dabei sind t,z und k beliebige Skalierungsfaktoren (t,z,kâˆˆR).


-> Determinantenverfahren 2x2 Matrix um die Eingenwerte zu berechenn

1 bis 2 Eigenwerte einsetzen und damit dann $lambda_1$ bzw. $lambda_2$ in die Matrix einsetzen und damit den Eigenvektor berechnen Ã¼ber den GauÃŸ-Algorythmus.

-> Determinantenverfahren 2x2 Matrix um die Eingenwerte zu berechenn

1 bis 2 Eigenwerte einsetzen und damit dann $lambda_1$ bzw. $lambda_2$ in die Matrix einsetzen und damit den Eigenvektor berechnen Ã¼ber den GauÃŸ-Algorythmus.

=== Komplexeres Beispiel
-> Determinantenverfahren 5x5 Matrix
-> Polynom vom 5. Grad -> Was nun?

== Python-Beispiele (Luis)

= *Anwendungen*

== Google PageRank (Michael)
Google PageRank ist ein Algorithmus, der von Google entwickelt wurde um Webseiten bei einer Suchanfrage zu bewerten und dementsprechend zu sortieren. Webseiten sind untereinander Ã¼ber Hyperlinks verbunden. Die Idee des Algorithmus ist nun, dass Webseiten, welche von mehreren anderen Webseiten referenziert werden als wichtiger eingestuft werden. Diese Wichtigkeit wird durch einen mathematischen Ansatz von Eigenwerten und Eigenvektoren definiert.

Man kann sich das Verfahren gut als einen gerichteten Graphen vorstellen, bei dem Webseiten durch Knoten dargestellt werden und Kanten die Hyperlinks darstellen. Die Rang einer Webseite wird dann iterativ berechnet, wobei beachtet wird, wie viele eingehende Links diese Webseite hat und welcher Rang die Webseiten haben die diese Webseite referenzieren.

Hier ist ein Beispiel eines Graphen
#import "@preview/diagraph:0.3.3" : *
$
#raw-render(
```dot
digraph PageRank {
  rankdir=LR;
  splines=ortho;
  overlap=true;
  nodesep=0.75;
  ranksep=0.75;


  { rank = min; Facebook; THWS; }

  { rank = same; Youtube; }

  { rank = max; Amazon; Netflix; }

  Facebook -> { Youtube Amazon THWS };
  Youtube  -> { Facebook THWS Netflix };
  Amazon   -> { Netflix Youtube };
  THWS     -> { Youtube Facebook };
  Netflix  -> { Amazon };
}
```)
$

=== Verfahren
Die Grundlegende Idee hinter dem PageRank Algorithmus ist durch eine Markov-Kette definiert. Das Internet ist als groÃŸer Graph dargestellt, bei dem jede Webseite ein State einer Markov-Kette ist. Ein Nutzer (oder auch "random surfer") kann sich von einer Webseite zur nÃ¤chsten Ã¼ber das klicken von links bewegen. Die Wahrscheinlichkeit, dass der Nutzer nach einer Anzahl an klicks auf einer gewissen Webseite landet, ist abhÃ¤Ã¼ngig von der Struktur des Internets und der Hyperlinkverteilung.

==== Markov-Ketten
In einer Markov-Kette werden die Ãœbergangswahrscheinlichkeiten zwischen ZustÃ¤nden (in diesem Fall Seiten) durch eine Ãœbergangsmatrix dargestellt. Die Elemente dieser Matrix stellen die Wahrscheinlichkeit dar, von einer Seite zu einer anderen zu wechseln. Um den PageRank zu berechnen, wird die Ãœbergangsmatrix auf aufeinanderfolgende Potenzen erhÃ¶ht und das Eigenwertproblem gelÃ¶st, um den stabilen Zustand dieses Prozesses zu finden.

Der Prozess kann mathematisch wie folgt dargestellt werden:

$
p = M p
$

Wobei:
    - *$p$* ist der PageRank-Vektor (die Wahrscheinlichkeitsverteilung Ã¼ber die Seiten).
    - *$M$* ist die Ãœbergangsmatrix, die die Webstruktur darstellt.

Da das Web jedoch extrem groÃŸ ist und viele Seiten mit unterschiedlichen Linkstrukturen enthÃ¤lt, ist die direkte Berechnung des PageRank unpraktisch. Stattdessen wird eine NÃ¤herungsmethode namens Power Iteration verwendet, um den Eigenvektor zu ermitteln, der dem Eigenwert 1 (dem Haupteigenvektor) entspricht, der die stationÃ¤re Verteilung eines zufÃ¤lligen Surfers darstellt.

=== ZurÃ¼ck zum Eigenwertproblem
Der LÃ¶sungsprozess fÃ¼r PageRank kann auf ein Eigenwertproblem reduziert werden. Aus der linearen Algebra wissen wir, dass, wenn eine Matrix *$M$* einen Eigenwert von 1 hat, es einen von Null verschiedenen Eigenvektor *$p$* gibt, so dass:

$
M p = p
$

Das bedeutet, dass der Eigenvektor $p$ der Gleichgewichtsvektor ist, der sich nach wiederholter Anwendung der Ãœbergangsmatrix $M$ nicht Ã¤ndert. Im Zusammenhang mit PageRank entspricht dieser Eigenvektor der relativen Wichtigkeit oder dem Ranking jeder Seite im Web.

Der PageRank-Vektor kann durch die folgenden Schritte ermittelt werden:

- *Initialisierung:* Begonnen wird mit einer beliebigen Wahrscheinlichkeitsverteilung Ã¼ber alle Seiten, hÃ¤ufig eine Gleichverteilung, bei der jede Seite die gleiche Wahrscheinlichkeit hat.

- *Iteration:* Der aktuelle Wahrscheinlichkeitsvektor wird mit der Ãœbergangsmatrix *$M$* multipliziert. Das passiert so lange, bis der Vektor zu einem stabilen Zustand konvergiert.

- *KonvergenzprÃ¼fung:* Die Iteration wird so lange fortgesetzt, bis die Ã„nderung des PageRank-Vektors zwischen zwei aufeinanderfolgenden Iterationen kleiner als ein vorgegebener Schwellenwert ist d.h. bis der Algorythmus also konvergiert hat.

In der Praxis bedeutet dies, dass der grÃ¶ÃŸte Eigenwert (*1*) der Ãœbergangsmatrix *$M$* gesucht wird. Der entsprechende Eigenvektor stellt die relative Wichtigkeit (PageRank) der einzelnen Webseiten dar.

=== Markov-Ketten und Eigenwertproblem

Die Verbindung zum Eigenwertproblem ergibt sich daraus, dass die PageRank-Berechnung auf der Suche nach dem dominanten Eigenvektor einer stochastischen Matrix (der Ãœbergangsmatrix des Webgraphen) beruht.

Die Ãœbergangsmatrix *$M$* ist eine stochastische Matrix, was bedeutet, dass jede Spalte die Summe 1 ergibt und ihre Eigenwerte innerhalb des Einheitskreises in der komplexen Ebene liegen. Wenn man den Eigenvektor findet, der dem Eigenwert 1 entspricht, kann man die stationÃ¤re Verteilung der Markov-Kette berechnen, was genau das ist, was PageRank berechnet.

Betrachten wir das Beispiel des oberen Graphen:
$
#raw-render(
```dot
digraph PageRank {
  rankdir=LR;
  splines=ortho;
  overlap=true;
  nodesep=0.75;
  ranksep=0.75;


  { rank = min; Facebook; THWS; }

  { rank = same; Youtube; }

  { rank = max; Amazon; Netflix; }

  Facebook -> { Youtube Amazon THWS };
  Youtube  -> { Facebook THWS Netflix };
  Amazon   -> { Netflix Youtube };
  THWS     -> { Youtube Facebook };
  Netflix  -> { Amazon };
}
```)
$

Die Pfeile zeigen an, welche Websites Links zu anderen Websites haben. Ziel ist es, dasselbe Konzept des PageRank mit Hilfe von Eigenwerten und Eigenvektoren anzuwenden, um die Bedeutung jeder Website auf der Grundlage der Linkstruktur zu berechnen.

Die Ãœbergangsmatrix *$M$* ist eine stochastische Matrix, was bedeutet, dass jede Spalte die Summe *1* ergibt und ihre Eigenwerte innerhalb des Einheitskreises in der komplexen Ebene liegen. Wenn man den Eigenvektor findet, der dem Eigenwert 1 entspricht, kann man die stationÃ¤re Verteilung der Markov-Kette berechnen, was genau das ist, was PageRank berechnet.

Wir kÃ¶nnen den Graphen als Ãœbergangsmatrix *$M$* darstellen, wobei jede Zeile und Spalte einer der Websites entspricht. Der Wert in Zeile *$i$* und Spalte *$j$* steht fÃ¼r die Wahrscheinlichkeit, dass ein zufÃ¤lliger Surfer von Website *$i$* zu Website *$j$* wechselt.

In diesem Fall kÃ¶nnen wir von einem einfachen Ansatz ausgehen, bei dem eine Website mit mehreren ausgehenden Links die gleiche Wahrscheinlichkeit auf ihre Links verteilt. Websites ohne ausgehende Links werden mit einem Teleportationsfaktor behandelt.

Die Ãœbergangsmatrix *$M$* aus der Grundlage des Graphen:

$
M = mat(
  -,  F,    Y,    A,    T,    N;
  F,  0,    1/3,  1/3,  1/3,  0;
  Y,  1/3,  0,    0,    1/3,  1/3;
  A,  0,    1/2,  0,    0,    1/2;
  T,  1/2,  1/2,  0,    0,    0;
  N,  0,    0,    1,    0,    0;
)
$

Zur Berechnung des stationÃ¤ren Vektors (PageRank-Vektor) wird der Eigenvektor mit dem Eigenwert 1 ermittelt. Dazu kann die Potenziteration oder ein anderer Eigenwertalgorithmus verwendet werden. Der resultierende PageRank-Vektor gibt die relative Bedeutung jeder Seite im Netz an.

Der PageRank entspricht der stationÃ¤ren Verteilung $p$, die die Gleichung

$
p = p M
$

erfÃ¼llt. In dem Fall ist $p = (p_F, p_Y, p_A, p_T, p_N)$ ein Zeilenvektor mit $ sum_i p_i = 1 $

Mathematisch ist dies also das Eigenwertproblem:

$
p(M - I) = 0, lambda=1
$

Aus der Ãœbergangsmatrix lÃ¤sst sich folgendes Gleichungssystem abbliden:

$
"LGS"(M) = cases(
  p_F = 1/3p_Y + 1/2p_T,
  p_Y = 1/3p_F + 1/2p_A + 1/2p_T,
  p_A = 1/3p_F + 1p_N ,
  p_T = 1/3p_F + 1/3p_Y,
  p_N = 1/3p_Y + 1/2p_A,
  p_F + p_Y + p_A + p_T + p_N = 1
)
$

Dieses Gleichungssystem lÃ¤sst sich jetzt anhand von unter anderem der Power-Iteration lÃ¶sen. Power-Iteration ist ein Algorythmus, der es einfach macht den betragsgrÃ¶ÃŸten Eigenwert $lambda_max$ und den zugehÃ¶rigen Eigenvektor $v_max$ einer reellen Matrix $M âˆˆ RR^(n Ã— n)$ zu approximieren. Die Power-Iteration nÃ¤hert sich diesem Vektor durch sukzessives Anwenden von $M$. ZunÃ¤chst muss der Initialvektor gewÃ¤hlt werden. Hier wird der Vektor $p^(0) = ( 1/n, 1/n, ... , 1/n), p^(0) âˆˆ RR^n$  wobei $n$ die Dimension der Matrix $M$ ist. In unserem Fall ist das also der Vektor:

$
p^(0) = (1/5, 1/5, 1/5, 1/5, 1/5)
$

AnschlieÃŸend muss nun wie oben beschrieben der Zeilenvektor $p^(k)$ mit der Matrix $M$ iterativ multipliziert werden, was sich mathematisch so darstellen lÃ¤sst.

$
p^(k+1) = p^(k)M
$

Ãœblicherweise wird zusÃ¤tzlich ein DÃ¤mpfungsfaktor $alpha âˆˆ (0,1)$ verwendet, sodass auch "Dangling Nodes", also Webseiten ohne ausgehende Hyperlinks korrekt behandelt werden und das Verfahren effizienter konvergiert. Das lÃ¤sst sich dann wiefolgt darstellen:

$
p^(k+1) = alpha p^(k) M + (1-a)/n (1, 1, ... 1)
$

Der Term $(1-alpha)/n (1, 1, ... 1)$ verteilt die restliche Wahrscheinlichkeit auf alle Seiten, wodurch der DÃ¤mpfungsfaktor erfolgreich eingebunden ist.

Dieser Prozess wird nun so lange wiederholt, bis ein der Algorythmus konvergiert. Das wird anhand einen Abbruchkriteriums festgelegt. 

$
Delta^k = || p^(k+1) -p^(k) ||_1 = sum_i |p_i^(k+1)-p_i^(k)|
$

Wenn $Delta^k < epsilon$ (z.B. $10^(-6)$), wird die Iteration abgebrochen und $p = p^(k+1)$ gesetzt.

Jetzt wenden wir dieses Prozedere auf unser Ãœbergangsmatrix $M$ an. Die Initialisierung von $p$:
$
p^(0) = (1/5, 1/5, 1/5, 1/5, 1/5)
$

1. Iteration:

$
p^(0)M= (0.166667,0.266667,0.266667,0.133333,0.166667)
$

DÃ¤mpfung und Teleportation einbinden:

$
p^(1) = 0.85(0.166667, 0.266667, 0.266667, 0.133333, 0.166667) + 0.15(0.2, 0.2, 0.2, 0.2, 0.2) 
$

$
= (0.85 * 0.166667 + 0.03,â€…...â€‰) = (0.171667,â€…0.256667,â€Š0.256667,â€Š0.143333,â€Š0.171667) 
$

2. Iteration:

$
p^(1)M â‰ˆ (0.157222, 0.257222, 0.228889, 0.142778, 0.213889)
$

DÃ¤mpfung und Teleportation

$
p(2)=0.85(0.157222,0.257222,0.228889,0.142778,0.213889)+0.15(0.2,0.2,0.2,0.2,0.2)
$

$
â‰ˆ(0.163639,â€…â€Š0.248639,â€…â€Š0.224556,â€…â€Š0.151361,â€…â€Š0.211806)
$

Im Beispiel unserer Ãœbergangsmatrix $M$ wÃ¼rden folgende Wichtigkeiten bzw. RÃ¤nge der Webseiten nach 20 Iterationen herauskommen:

$
#table(
  columns: 2,
  table.header(
    [Website],
    [PageRank],
  ),
  
  [Facebook], [0.1475],
  [YouTube],  [0.2459],
  [Amazon],   [0.2623],
  [THWS],     [0.1311],
  [Netflix],  [0.2131],
)
$
== Kompression & Bildverarbeitung

== Datenanalyse (Wichtig fÃ¼r KI)

== Ausblick in die Wissenschaft

//Notizen

Eigenwerte und Eigenvektoren

Eine Matrix *$A$* kann auf einen Spaltenvektor *$x$* angewendet werden, was
eine *lineare Transformation* von *$x$* darstellt.

Es gibt eine besondere Form dieser Transformation:

$ A x = lambda x $

Dabei gilt:
- *$A$* ist eine $n times n$ Matrix.
- *$x$* ist ein $n times 1$ Spaltenvektor (wobei $x eq.not 0$).
- *$lambda$* ist ein Skalar.

Jeder Wert von *$lambda$*, der diese Gleichung erfÃ¼llt, wird als *Eigenwert* der Matrix *$A$* bezeichnet.
Der zugehÃ¶rige Vektor *$x$* wird als *Eigenvektor* zum Eigenwert *$lambda$* bezeichnet.

Motivation hinter Eigenwerten und Eigenvektoren

- *Grundidee:*
  - Eigenwerte und Eigenvektoren helfen uns, die Eigenschaften von linearen Transformationen besser zu verstehen und Probleme zu vereinfachen.

- *Transformation eines Vektors:*
  - Ein Vektor *$x$* kann durch eine Matrix *$A$* in einen neuen Vektor *$A x$* umgewandelt werden.
  - Diese Transformation kann die LÃ¤nge (Skalierung) und/oder die Richtung (Drehung) des Vektors Ã¤ndern.

- *Besondere Vektoren - Eigenvektoren:*
  - FÃ¼r bestimmte Vektoren ist der Transformationseffekt *nur eine Skalierung* (Strecken, Komprimieren oder Spiegeln), ohne die Richtung zu Ã¤ndern.
  - Diese Vektoren werden als *Eigenvektoren* bezeichnet.

- *Skalierungsfaktoren - Eigenwerte:*
  - Die Werte, um die ein Eigenvektor skaliert wird, nennt man *Eigenwerte $Î»$*.

- *Zusammenfassung:*
  - *Eigenvektoren:* Vektoren, die bei der Transformation nur gestreckt oder komprimiert werden.
  - *Eigenwerte:* Faktoren, die die LÃ¤nge des Eigenvektors bestimmen.

#pagebreak()

Python-Beispiel: Eigenvektoren plotten

Sehen wir uns ein Beispiel an, um die Transformation eines Vektors durch eine Matrix zu visualisieren.

Wir plotten den Vektor $x = mat(1; 1)$ und den transformierten Vektor $b = A x$, wobei die
Matrix $A$ gegeben ist als $A = mat(2, 0; 0, 1)$.

(Jupyter-Notebook)

#let plot_example1 = read("plot_example1.py")
#raw(plot_example1, lang: "python")

Grafik einfÃ¼gen
image("eigenvector_plot.png", width: 80%)

Plot-Ergebnis

Der Plot zeigt die beiden Vektoren:

- *Schwarzer Vektor:* UrsprÃ¼nglicher Vektor $x$
- *GrÃ¼ner Vektor:* Transformierter Vektor $b$

Die Grafik veranschaulicht die *Streckung* und die minimale *Rotation* des Vektors nach der Transformation.

#pagebreak()

Grundlagen

$
  mat(
    1, 2, ..., 10;
    2, 2, ..., 10;
    dots.v, dots.v, dots.down, dots.v;
    10, 10, ..., 10;
  )
$

Matrizen Wiederholung

Vektorraum und Vektoren

Matrix Generell

Determinante

Python Beispiel: Multiplikation / Determinante mit numpy

Matrix Multiplikation:

```python
import numpy as np

a = np.array([[1, 0],
              [0, 1]])

b = np.array([[4, 1],
              [2, 2]])

result = np.matmul(a, b)
```

Charakeristisches Polynom

Spektralsatz (Wichtig fÃ¼r z.B. Bildverarbeitung/Kompression)

Berechnung von Eigenwerten & Eigenvektoren

Python Beispiel: Berechnung

Geometrische Anschauung

Anwendungen



Anwendungen in der Numerik

Google PageRank

Allgemein

Verfahren

Markov-Ketten

ZurÃ¼ck zum Eigenwertproblem


Kompression & Bildverarbeitung

Datenanalyse (Wichtig fÃ¼r KI)



#pagebreak()

= *Anhang*

== Code zu @plot_eigen1

#let plot_eigen1 = read("plot_eigen1.py")
#raw(plot_eigen1, lang: "python")
